{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXfvihT3DzSF"
      },
      "source": [
        "# Importing packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Dp3iL5aUECDL"
      },
      "outputs": [],
      "source": [
        "# utility packages\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "# tensorflow \n",
        "import tensorflow as tf\n",
        "\n",
        "# random package\n",
        "import random \n",
        "\n",
        "# warnings \n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZON9yN0ESBI"
      },
      "source": [
        "### Checking for GPU usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pQjQfwOE7CJ",
        "outputId": "f832bbe7-e573-4724-bae8-efdc762a99c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':  \n",
        "  print(f'No GPU was found.')\n",
        "else:\n",
        "  print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64hMBWIjE7fa"
      },
      "source": [
        "### Loading the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g697hdknFCMm"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42QBUpOaFGZn",
        "outputId": "b63a1d3b-d66e-42c3-faea-980dcb75004c"
      },
      "outputs": [],
      "source": [
        "### unzipping the mnist dataset \n",
        "\n",
        "(xTrain,yTrainLabel),(xTest,yTestLabel) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5vSZwC8KC6G"
      },
      "source": [
        "### Manipulation the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BFNNt2RtFQrt"
      },
      "outputs": [],
      "source": [
        "### One-hot encoding the training and test labels\n",
        "\n",
        "classes = 10 # 0-9 categories for the num_classes parameter\n",
        "\n",
        "# training\n",
        "yTrainCat = tf.keras.utils.to_categorical(y = yTrainLabel, num_classes = classes, dtype = 'float32')\n",
        "\n",
        "# testing\n",
        "yTestCat = tf.keras.utils.to_categorical(y = yTestLabel, num_classes = classes, dtype = 'float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRZlRICcGTRY",
        "outputId": "11c26d3d-67d6-43f3-e36f-6a966de9c827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset shape: (60000, 28, 28) | Training dataset datatype: uint8 \n",
            "Testing dataset shape: (10000, 28, 28) | Testing dataset datatype: uint8\n",
            "Training dataset pixel range: (0, 255) | Testing dataset pixel range: (0, 255)\n"
          ]
        }
      ],
      "source": [
        "### Shape and datatype of MNIST dataset\n",
        "\n",
        "print(f'Training dataset shape: {xTrain.shape} | Training dataset datatype: {xTrain.dtype} \\nTesting dataset shape: {xTest.shape} | Testing dataset datatype: {xTrain.dtype}')\n",
        "print(f'Training dataset pixel range: {(np.min(xTrain),np.max(xTrain))} | Testing dataset pixel range: {(np.min(xTest),np.max(xTest))}' )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HbyMhI6JtzX"
      },
      "source": [
        "**Need to reshape, change the datatype, and min_max scale the tensor values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OC_g8N5OK0Ao"
      },
      "outputs": [],
      "source": [
        "### Manipulate the training and testing input\n",
        "\n",
        "xTrain = (xTrain/255).astype('float32') # 0-1 scaled, datatype is now a float\n",
        "xTest = (xTest/255).astype('float32') # 0-1 scaled, datatype is now a float"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kd4V3LGLuHl"
      },
      "source": [
        "### Build and establish the model\n",
        "The upper restriciton on the number of params: 200,000. I will initialize a function api model using keras.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4trvpgZYMM0b"
      },
      "outputs": [],
      "source": [
        "### Model ### --- layer adjustments\n",
        "\n",
        "#creating lists for column names \n",
        "model_names = []\n",
        "first_layer_nodes = []\n",
        "second_layer_nodes = []\n",
        "third_layer_nodes = []\n",
        "first_layer_activation = []\n",
        "second_layer_activation = []\n",
        "third_layer_activation = []\n",
        "output_layer_activation = []\n",
        "training_loss = []\n",
        "training_accuracy = []\n",
        "validation_loss = []\n",
        "validation_accuracy = []\n",
        "batch_size = [] \n",
        "num_of_epochs = []\n",
        "test_loss = []\n",
        "test_accuracy = []\n",
        "optimizer_function = []\n",
        "loss_function = []\n",
        "\n",
        "def model_builder1(training_dataset = xTrain, \n",
        "                  training_labels = yTrainCat, \n",
        "                  testing_dataset = xTest, \n",
        "                  testing_labels = yTestCat):\n",
        "  \n",
        "  ### ----- Local Variables ----- ###\n",
        "  counter = 0\n",
        "  firstActive = 'relu'\n",
        "  secondActive = 'relu'\n",
        "  thirdActive = 'relu'\n",
        "  outputActive = 'sigmoid'\n",
        "  firstLayerNodes = 128 # number of nodes in first layer list\n",
        "  secondLayerNodes = [150,200,250]  # number of nodes in second layer list\n",
        "  thirdLayerNodes = [150,200,250] # number of nodes in the third layer list\n",
        "\n",
        "  ### ----- Creating the full-dense network ----- ###\n",
        "\n",
        "  for i in secondLayerNodes: # will iterate through second layer nodes\n",
        "    for j in thirdLayerNodes: # iterate through third layer nodes\n",
        "      counter += 1\n",
        "      inputLayer = tf.keras.Input(shape = xTrain.shape[1:], name = 'input_layer')\n",
        "      flattenLayer = tf.keras.layers.Flatten(name = 'flatten_layer')(inputLayer)\n",
        "      denseLayer1 = tf.keras.layers.Dense(units = firstLayerNodes, activation = firstActive, name = 'dense_layer_1')(flattenLayer)\n",
        "      denseLayer2 = tf.keras.layers.Dense(units = i, activation = secondActive, name = 'dense_layer_2')(denseLayer1)\n",
        "      dropoutLayer = tf.keras.layers.Dropout(0.2, name = 'dropout_layer')(denseLayer2)\n",
        "      denseLayer3 = tf.keras.layers.Dense(units = j, activation = thirdActive, name = 'dense_layer_3')(dropoutLayer)\n",
        "      outputLayer = tf.keras.layers.Dense(units = 10, activation = outputActive, name = 'output_layer')(denseLayer3)\n",
        "      \n",
        "      # appending all hyperparameters into lists\n",
        "      first_layer_nodes.append(firstLayerNodes) # append number of first layer nodes \n",
        "      second_layer_nodes.append(i) # append number of second layer nodes\n",
        "      third_layer_nodes.append(j) # append number of third layer nodes\n",
        "      first_layer_activation.append(firstActive) # append first layer activation function\n",
        "      second_layer_activation.append(secondActive) # append second layer activation function\n",
        "      third_layer_activation.append(thirdActive) # append third layer activation function\n",
        "      output_layer_activation.append(outputActive) # append output layer activation function\n",
        "\n",
        "      modelName = f'NN_Model{str(counter)}' # generating model names to put into list\n",
        "      model_names.append(modelName) # appending name to model_names list\n",
        "\n",
        "      model = tf.keras.Model(inputs = inputLayer, outputs = outputLayer, name = modelName)\n",
        "      \n",
        "      ### ----- Model Parameters ----- ###\n",
        "\n",
        "      optFunction = 'adam' # optimizer function\n",
        "      lossFunction = 'binary_crossentropy' # loss function\n",
        "\n",
        "      optimizer_function.append(str(optFunction)) # append optimizer function\n",
        "      loss_function.append(str(lossFunction)) # append loss function\n",
        "\n",
        "      ### ----- Compiler ----- ###\n",
        "      model.compile(\n",
        "        optimizer = optFunction,\n",
        "        loss = lossFunction,\n",
        "        metrics = 'accuracy'\n",
        "      )\n",
        "\n",
        "      ##### ----- Fitting the model ----- ###\n",
        "      print(f'first layer: 128 | second layer: {i} | third layer: {j}') \n",
        "\n",
        "      # tf.random.set_seed(42)\n",
        "\n",
        "      bSize = 128\n",
        "      epoch = 10\n",
        "      vSplit = 0.1\n",
        "      trainModel = model.fit(\n",
        "          x = xTrain,\n",
        "          y = yTrainCat,\n",
        "          batch_size = bSize,\n",
        "          epochs = epoch,\n",
        "          validation_split = vSplit\n",
        "      )\n",
        "\n",
        "      batch_size.append(bSize) # append batch size\n",
        "      num_of_epochs.append(epoch) # append number of epochs \n",
        "      \n",
        "      ### ----- Results ----- ###\n",
        "      # appending the validation accuracy into the list\n",
        "      training_loss.append(min(trainModel.history['loss'])) # append training loss\n",
        "      training_accuracy.append(max(trainModel.history['accuracy'])) # append training accuracy\n",
        "      validation_loss.append(min(trainModel.history['val_loss'])) # append validation loss\n",
        "      validation_accuracy.append(max(trainModel.history['val_accuracy'])) # append validation accuracy \n",
        "                                  \n",
        "      # appending the test accuracy into the list\n",
        "      finalResults = model.evaluate(xTest,yTestCat)\n",
        "\n",
        "      test_loss.append(finalResults[0]) # append test loss\n",
        "      test_accuracy.append(finalResults[1]) # append test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_builder2(training_dataset = xTrain, \n",
        "                  training_labels = yTrainCat, \n",
        "                  testing_dataset = xTest, \n",
        "                  testing_labels = yTestCat):\n",
        "  \n",
        "  ### ----- Local Variables ----- ###\n",
        "  counter = 9 #starting where model_builder1 left off\n",
        "  firstActive = 'gelu'\n",
        "  secondActive = 'gelu'\n",
        "  thirdActive = 'gelu'\n",
        "  outputActive = 'softmax'\n",
        "  firstLayerNodes = 128 # number of nodes in first layer list\n",
        "  secondLayerNodes = [175,200,225]  # number of nodes in second layer list\n",
        "  thirdLayerNodes = [175,200,225] # number of nodes in the third layer list\n",
        "  \n",
        "  ### ----- Creating the full-dense network ----- ###\n",
        "\n",
        "  for i in secondLayerNodes: # will iterate through second layer nodes\n",
        "    for j in thirdLayerNodes: # iterate through third layer nodes\n",
        "      counter += 1\n",
        "      inputLayer = tf.keras.Input(shape = xTrain.shape[1:], name = 'input_layer')\n",
        "      flattenLayer = tf.keras.layers.Flatten(name = 'flatten_layer')(inputLayer)\n",
        "      denseLayer1 = tf.keras.layers.Dense(units = firstLayerNodes, activation = firstActive, name = 'dense_layer_1')(flattenLayer)\n",
        "      denseLayer2 = tf.keras.layers.Dense(units = i, activation = secondActive, name = 'dense_layer_2')(denseLayer1)\n",
        "      dropoutLayer = tf.keras.layers.Dropout(rate = 0.2, name = 'dropout_layer')(denseLayer2)\n",
        "      denseLayer3 = tf.keras.layers.Dense(units = j, activation = thirdActive, name = 'dense_layer_3')(dropoutLayer)\n",
        "      outputLayer = tf.keras.layers.Dense(units = 10, activation = outputActive, name = 'output_layer')(denseLayer3)\n",
        "\n",
        "      # appending all hyperparameters into lists\n",
        "      first_layer_nodes.append(firstLayerNodes) # append number of first layer nodes \n",
        "      second_layer_nodes.append(i) # append number of second layer nodes\n",
        "      third_layer_nodes.append(j) # append number of third layer nodes\n",
        "      first_layer_activation.append(firstActive) # append first layer activation function\n",
        "      second_layer_activation.append(secondActive) # append second layer activation function\n",
        "      third_layer_activation.append(thirdActive) # append third lyaer activation function\n",
        "      output_layer_activation.append(outputActive) # append output layer activation function\n",
        "\n",
        "      modelName = f'NN_Model{str(counter)}' # generating model names to put into list\n",
        "      model_names.append(modelName) # appending name to model_names list\n",
        "\n",
        "      model = tf.keras.Model(inputs = inputLayer, outputs = outputLayer, name = modelName)\n",
        "\n",
        "      ### ----- Model Parameters ----- ###\n",
        "      if counter%2 == 0: \n",
        "        optFunction = 'adam' # optimizer function\n",
        "        lossFunction = 'binary_crossentropy' # loss function\n",
        "      else:\n",
        "        optFunction = 'SGD' # optimizer function\n",
        "        lossFunction = 'hinge' # loss function  \n",
        "\n",
        "      optimizer_function.append(str(optFunction)) # append optimizer function\n",
        "      loss_function.append(str(lossFunction)) # append loss function\n",
        "\n",
        "      ### ----- Compiler ----- ###\n",
        "      model.compile(\n",
        "        optimizer = optFunction,\n",
        "        loss = lossFunction,\n",
        "        metrics = 'accuracy'\n",
        "      )\n",
        "\n",
        "      ##### ----- Fitting the model ----- ###\n",
        "      print(f'first layer: 128 | second layer : {i}, third layer {j}') \n",
        "\n",
        "      tf.random.set_seed(42)\n",
        "\n",
        "      bSize = 128\n",
        "      epoch = 10\n",
        "      vSplit = 0.1\n",
        "      trainModel = model.fit(\n",
        "          x = xTrain,\n",
        "          y = yTrainCat,\n",
        "          batch_size = bSize,\n",
        "          epochs = epoch,\n",
        "          validation_split = vSplit\n",
        "      )\n",
        "\n",
        "      batch_size.append(bSize) # append batch size\n",
        "      num_of_epochs.append(epoch) # append number of epochs \n",
        "      \n",
        "      ### ----- Results ----- ###\n",
        "      # appending the validation accuracy into the list\n",
        "      training_loss.append(min(trainModel.history['loss'])) # append training loss\n",
        "      training_accuracy.append(max(trainModel.history['accuracy'])) # append training accuracy\n",
        "      validation_loss.append(min(trainModel.history['val_loss'])) # append validation loss\n",
        "      validation_accuracy.append(max(trainModel.history['val_accuracy'])) # append validation accuracy \n",
        "                                 \n",
        "      # appending the test accuracy into the list\n",
        "      finalResults = model.evaluate(xTest,yTestCat)\n",
        "\n",
        "      test_loss.append(finalResults[0]) # append test loss\n",
        "      test_accuracy.append(finalResults[1]) # append test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_builder3(training_dataset = xTrain, \n",
        "                  training_labels = yTrainCat, \n",
        "                  testing_dataset = xTest, \n",
        "                  testing_labels = yTestCat):\n",
        "  \n",
        "  ### ----- Local Variables ----- ###\n",
        "  counter = 18 #starting where model_builder2 left off\n",
        "  firstActive = 'elu'\n",
        "  secondActive = 'elu'\n",
        "  thirdActive = 'elu'\n",
        "  outputActive = 'swish'\n",
        "  firstLayerNodes = 128 # number of nodes in first layer list\n",
        "  secondLayerNodes = [190,200,210]  # number of nodes in second layer list\n",
        "  thirdLayerNodes = [190,200,210] # number of nodes in the third layer list\n",
        "\n",
        "  ### ----- Creating the full-dense network ----- ###\n",
        "\n",
        "  for i in secondLayerNodes: # will iterate through second layer nodes\n",
        "    for j in thirdLayerNodes: # iterate through third layer nodes\n",
        "      counter += 1\n",
        "      inputLayer = tf.keras.Input(shape =xTrain.shape[1:], name = 'input_layer')\n",
        "      flattenLayer = tf.keras.layers.Flatten(name = 'flatten_layer')(inputLayer)\n",
        "      denseLayer1 = tf.keras.layers.Dense(units = firstLayerNodes, activation = firstActive, name = 'dense_layer_1')(flattenLayer)\n",
        "      denseLayer2 = tf.keras.layers.Dense(units = i, activation = secondActive, name = 'dense_layer_2')(denseLayer1)\n",
        "      dropoutLayer = tf.keras.layers.Dropout(rate = 0.2, name = 'dropout_layer')(denseLayer2)\n",
        "      denseLayer3 = tf.keras.layers.Dense(units = j, activation = thirdActive, name = 'dense_layer_3')(dropoutLayer)\n",
        "      outputLayer = tf.keras.layers.Dense(units = 10, activation = outputActive, name = 'output_layer')(denseLayer3)\n",
        "\n",
        "      # appending all hyperparameters into lists\n",
        "      first_layer_nodes.append(firstLayerNodes) # append number of first layer nodes \n",
        "      second_layer_nodes.append(i) # append number of second layer nodes\n",
        "      third_layer_nodes.append(j) # append number of third layer nodes\n",
        "      first_layer_activation.append(firstActive) # append first layer activation function\n",
        "      second_layer_activation.append(secondActive) # append second layer activation function\n",
        "      third_layer_activation.append(thirdActive) # append third layer activation function\n",
        "      output_layer_activation.append(outputActive) # append output layer activation function\n",
        "\n",
        "      modelName = f'NN_Model{str(counter)}' # generating model names to put into list\n",
        "      model_names.append(modelName) # appending name to model_names list\n",
        "\n",
        "      model = tf.keras.Model(inputs = inputLayer, outputs = outputLayer, name = modelName)\n",
        "\n",
        "      ### ----- Model Parameters ----- ###\n",
        "      if counter%2 == 0: \n",
        "        optFunction = 'RMSprop' # optimizer function\n",
        "        lossFunction = 'hinge' # loss function\n",
        "      else:\n",
        "        optFunction = 'adam' # optimizer function\n",
        "        lossFunction = 'squared_hinge' # loss function  \n",
        "\n",
        "      optimizer_function.append(str(optFunction)) # append optimizer function\n",
        "      loss_function.append(str(lossFunction)) # append loss function\n",
        "\n",
        "      ### ----- Compiler ----- ###\n",
        "      model.compile(\n",
        "        optimizer = optFunction,\n",
        "        loss = lossFunction,\n",
        "        metrics = 'accuracy'\n",
        "      )\n",
        "\n",
        "      ##### ----- Fitting the model ----- ###\n",
        "      print(f'first layer: 128 | second layer {i} | third layer {j}') \n",
        "\n",
        "      tf.random.set_seed(42)\n",
        "\n",
        "      bSize = 128\n",
        "      epoch = 10\n",
        "      vSplit = 0.1\n",
        "      trainModel = model.fit(\n",
        "          x = xTrain,\n",
        "          y = yTrainCat,\n",
        "          batch_size = bSize,\n",
        "          epochs = epoch,\n",
        "          validation_split = vSplit\n",
        "      )\n",
        "\n",
        "      batch_size.append(bSize) # append batch size\n",
        "      num_of_epochs.append(epoch) # append number of epochs \n",
        "      \n",
        "      ### ----- Results ----- ###\n",
        "      # appending the validation accuracy into the list\n",
        "      training_loss.append(min(trainModel.history['loss'])) # append training loss\n",
        "      training_accuracy.append(max(trainModel.history['accuracy'])) # append training accuracy\n",
        "      validation_loss.append(min(trainModel.history['val_loss'])) # append validation loss\n",
        "      validation_accuracy.append(max(trainModel.history['val_accuracy'])) # append validation accuracy \n",
        "                                 \n",
        "      # appending the test accuracy into the list\n",
        "      finalResults = model.evaluate(xTest,yTestCat)\n",
        "\n",
        "      test_loss.append(finalResults[0]) # append test loss\n",
        "      test_accuracy.append(finalResults[1]) # append test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87ukMxm5qPTy"
      },
      "source": [
        "### Running the hyper-parameter tunned Model ### \n",
        "This first round of models controls the number of nodes/perceptrons within the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84UX7eL7qOdx",
        "outputId": "d2950b37-228d-4729-ed8a-77909cedefd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first layer: 128 | second layer: 150 | third layer: 150\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 4s 7ms/step - loss: 0.0879 - accuracy: 0.8529 - val_loss: 0.0256 - val_accuracy: 0.9602\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0280 - accuracy: 0.9549 - val_loss: 0.0178 - val_accuracy: 0.9718\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0189 - accuracy: 0.9691 - val_loss: 0.0152 - val_accuracy: 0.9743\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0143 - accuracy: 0.9777 - val_loss: 0.0137 - val_accuracy: 0.9788\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0116 - accuracy: 0.9818 - val_loss: 0.0130 - val_accuracy: 0.9787\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0093 - accuracy: 0.9850 - val_loss: 0.0119 - val_accuracy: 0.9790\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0076 - accuracy: 0.9880 - val_loss: 0.0126 - val_accuracy: 0.9800\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0066 - accuracy: 0.9892 - val_loss: 0.0139 - val_accuracy: 0.9775\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0055 - accuracy: 0.9913 - val_loss: 0.0142 - val_accuracy: 0.9787\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0049 - accuracy: 0.9923 - val_loss: 0.0153 - val_accuracy: 0.9772\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0163 - accuracy: 0.9764\n",
            "first layer: 128 | second layer: 150 | third layer: 200\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0835 - accuracy: 0.8545 - val_loss: 0.0258 - val_accuracy: 0.9587\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0271 - accuracy: 0.9560 - val_loss: 0.0177 - val_accuracy: 0.9703\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0187 - accuracy: 0.9704 - val_loss: 0.0145 - val_accuracy: 0.9780\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0141 - accuracy: 0.9776 - val_loss: 0.0150 - val_accuracy: 0.9752\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0111 - accuracy: 0.9827 - val_loss: 0.0133 - val_accuracy: 0.9782\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0089 - accuracy: 0.9861 - val_loss: 0.0132 - val_accuracy: 0.9787\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0071 - accuracy: 0.9892 - val_loss: 0.0147 - val_accuracy: 0.9775\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0065 - accuracy: 0.9900 - val_loss: 0.0122 - val_accuracy: 0.9815\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0053 - accuracy: 0.9915 - val_loss: 0.0133 - val_accuracy: 0.9817\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0047 - accuracy: 0.9925 - val_loss: 0.0143 - val_accuracy: 0.9808\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0160 - accuracy: 0.9775\n",
            "first layer: 128 | second layer: 150 | third layer: 250\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0806 - accuracy: 0.8605 - val_loss: 0.0236 - val_accuracy: 0.9618\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0262 - accuracy: 0.9578 - val_loss: 0.0177 - val_accuracy: 0.9730\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0188 - accuracy: 0.9701 - val_loss: 0.0145 - val_accuracy: 0.9748\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0141 - accuracy: 0.9783 - val_loss: 0.0139 - val_accuracy: 0.9787\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0114 - accuracy: 0.9818 - val_loss: 0.0147 - val_accuracy: 0.9753\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0095 - accuracy: 0.9843 - val_loss: 0.0141 - val_accuracy: 0.9777\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0079 - accuracy: 0.9876 - val_loss: 0.0134 - val_accuracy: 0.9795\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0064 - accuracy: 0.9896 - val_loss: 0.0128 - val_accuracy: 0.9802\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0055 - accuracy: 0.9917 - val_loss: 0.0134 - val_accuracy: 0.9812\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0050 - accuracy: 0.9920 - val_loss: 0.0140 - val_accuracy: 0.9800\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0146 - accuracy: 0.9787\n",
            "first layer: 128 | second layer: 200 | third layer: 150\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0799 - accuracy: 0.8649 - val_loss: 0.0238 - val_accuracy: 0.9638\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0256 - accuracy: 0.9587 - val_loss: 0.0171 - val_accuracy: 0.9715\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0175 - accuracy: 0.9724 - val_loss: 0.0155 - val_accuracy: 0.9745\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0134 - accuracy: 0.9789 - val_loss: 0.0149 - val_accuracy: 0.9752\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0109 - accuracy: 0.9829 - val_loss: 0.0143 - val_accuracy: 0.9752\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0085 - accuracy: 0.9864 - val_loss: 0.0122 - val_accuracy: 0.9810\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0071 - accuracy: 0.9889 - val_loss: 0.0142 - val_accuracy: 0.9780\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0061 - accuracy: 0.9902 - val_loss: 0.0125 - val_accuracy: 0.9803\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0053 - accuracy: 0.9915 - val_loss: 0.0137 - val_accuracy: 0.9795\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0045 - accuracy: 0.9929 - val_loss: 0.0132 - val_accuracy: 0.9813\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0152 - accuracy: 0.9804\n",
            "first layer: 128 | second layer: 200 | third layer: 200\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.0815 - accuracy: 0.8611 - val_loss: 0.0224 - val_accuracy: 0.9662\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0253 - accuracy: 0.9584 - val_loss: 0.0170 - val_accuracy: 0.9708\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0175 - accuracy: 0.9718 - val_loss: 0.0138 - val_accuracy: 0.9755\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0133 - accuracy: 0.9795 - val_loss: 0.0135 - val_accuracy: 0.9765\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0103 - accuracy: 0.9832 - val_loss: 0.0120 - val_accuracy: 0.9788\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0085 - accuracy: 0.9865 - val_loss: 0.0130 - val_accuracy: 0.9782\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0069 - accuracy: 0.9891 - val_loss: 0.0130 - val_accuracy: 0.9800\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0061 - accuracy: 0.9903 - val_loss: 0.0130 - val_accuracy: 0.9790\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0049 - accuracy: 0.9922 - val_loss: 0.0135 - val_accuracy: 0.9775\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.0045 - accuracy: 0.9932 - val_loss: 0.0136 - val_accuracy: 0.9817\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0140 - accuracy: 0.9797\n",
            "first layer: 128 | second layer: 200 | third layer: 250\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 4s 7ms/step - loss: 0.0776 - accuracy: 0.8715 - val_loss: 0.0232 - val_accuracy: 0.9615\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0252 - accuracy: 0.9594 - val_loss: 0.0164 - val_accuracy: 0.9743\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0173 - accuracy: 0.9719 - val_loss: 0.0161 - val_accuracy: 0.9715\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0131 - accuracy: 0.9791 - val_loss: 0.0139 - val_accuracy: 0.9780\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0102 - accuracy: 0.9838 - val_loss: 0.0133 - val_accuracy: 0.9772\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0087 - accuracy: 0.9857 - val_loss: 0.0121 - val_accuracy: 0.9802\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0071 - accuracy: 0.9886 - val_loss: 0.0130 - val_accuracy: 0.9792\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0060 - accuracy: 0.9902 - val_loss: 0.0136 - val_accuracy: 0.9788\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0054 - accuracy: 0.9915 - val_loss: 0.0130 - val_accuracy: 0.9813\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0044 - accuracy: 0.9932 - val_loss: 0.0132 - val_accuracy: 0.9817\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0150 - accuracy: 0.9804\n",
            "first layer: 128 | second layer: 250 | third layer: 150\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 4s 7ms/step - loss: 0.0847 - accuracy: 0.8541 - val_loss: 0.0236 - val_accuracy: 0.9638\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0253 - accuracy: 0.9600 - val_loss: 0.0159 - val_accuracy: 0.9737\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0169 - accuracy: 0.9730 - val_loss: 0.0142 - val_accuracy: 0.9760\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0126 - accuracy: 0.9801 - val_loss: 0.0140 - val_accuracy: 0.9767\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0101 - accuracy: 0.9840 - val_loss: 0.0122 - val_accuracy: 0.9800\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0079 - accuracy: 0.9874 - val_loss: 0.0125 - val_accuracy: 0.9783\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0061 - accuracy: 0.9907 - val_loss: 0.0132 - val_accuracy: 0.9798\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0056 - accuracy: 0.9917 - val_loss: 0.0126 - val_accuracy: 0.9803\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0052 - accuracy: 0.9920 - val_loss: 0.0133 - val_accuracy: 0.9803\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0043 - accuracy: 0.9927 - val_loss: 0.0138 - val_accuracy: 0.9810\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0166 - accuracy: 0.9771\n",
            "first layer: 128 | second layer: 250 | third layer: 200\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0749 - accuracy: 0.8737 - val_loss: 0.0230 - val_accuracy: 0.9648\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0248 - accuracy: 0.9601 - val_loss: 0.0174 - val_accuracy: 0.9712\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0172 - accuracy: 0.9727 - val_loss: 0.0159 - val_accuracy: 0.9750\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0128 - accuracy: 0.9798 - val_loss: 0.0140 - val_accuracy: 0.9770\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0101 - accuracy: 0.9837 - val_loss: 0.0131 - val_accuracy: 0.9782\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0086 - accuracy: 0.9861 - val_loss: 0.0133 - val_accuracy: 0.9797\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0067 - accuracy: 0.9890 - val_loss: 0.0130 - val_accuracy: 0.9800\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0058 - accuracy: 0.9909 - val_loss: 0.0144 - val_accuracy: 0.9782\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0050 - accuracy: 0.9918 - val_loss: 0.0157 - val_accuracy: 0.9775\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0038 - accuracy: 0.9942 - val_loss: 0.0166 - val_accuracy: 0.9797\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0157 - accuracy: 0.9788\n",
            "first layer: 128 | second layer: 250 | third layer: 250\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0743 - accuracy: 0.8731 - val_loss: 0.0227 - val_accuracy: 0.9642\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0238 - accuracy: 0.9616 - val_loss: 0.0164 - val_accuracy: 0.9750\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0160 - accuracy: 0.9743 - val_loss: 0.0151 - val_accuracy: 0.9758\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0124 - accuracy: 0.9800 - val_loss: 0.0132 - val_accuracy: 0.9785\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0101 - accuracy: 0.9838 - val_loss: 0.0137 - val_accuracy: 0.9783\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0078 - accuracy: 0.9873 - val_loss: 0.0148 - val_accuracy: 0.9768\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0066 - accuracy: 0.9892 - val_loss: 0.0147 - val_accuracy: 0.9795\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0055 - accuracy: 0.9911 - val_loss: 0.0162 - val_accuracy: 0.9760\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0053 - accuracy: 0.9914 - val_loss: 0.0137 - val_accuracy: 0.9832\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.0046 - accuracy: 0.9925 - val_loss: 0.0137 - val_accuracy: 0.9803\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0153 - accuracy: 0.9778\n"
          ]
        }
      ],
      "source": [
        "if device_name != '/device:GPU:0':  \n",
        "  model_builder1()\n",
        "else:\n",
        "    with tf.device('GPU:0'):\n",
        "        model_builder1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first layer: 128 | second layer : 175, third layer 175\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.0836 - accuracy: 0.8613 - val_loss: 0.0265 - val_accuracy: 0.9590\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.0291 - accuracy: 0.9534 - val_loss: 0.0182 - val_accuracy: 0.9710\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0197 - accuracy: 0.9686 - val_loss: 0.0154 - val_accuracy: 0.9758\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0152 - accuracy: 0.9751 - val_loss: 0.0130 - val_accuracy: 0.9802\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0121 - accuracy: 0.9806 - val_loss: 0.0132 - val_accuracy: 0.9802\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0093 - accuracy: 0.9851 - val_loss: 0.0130 - val_accuracy: 0.9785\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0076 - accuracy: 0.9878 - val_loss: 0.0140 - val_accuracy: 0.9782\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0062 - accuracy: 0.9900 - val_loss: 0.0135 - val_accuracy: 0.9798\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0053 - accuracy: 0.9923 - val_loss: 0.0146 - val_accuracy: 0.9793\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0043 - accuracy: 0.9933 - val_loss: 0.0144 - val_accuracy: 0.9795\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0161 - accuracy: 0.9765\n",
            "first layer: 128 | second layer : 175, third layer 200\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 1.0796 - accuracy: 0.1573 - val_loss: 1.0795 - val_accuracy: 0.1950\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0794 - accuracy: 0.1861 - val_loss: 1.0793 - val_accuracy: 0.2243\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 1.0793 - accuracy: 0.2078 - val_loss: 1.0791 - val_accuracy: 0.2572\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0791 - accuracy: 0.2315 - val_loss: 1.0790 - val_accuracy: 0.2817\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0789 - accuracy: 0.2527 - val_loss: 1.0788 - val_accuracy: 0.3030\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0787 - accuracy: 0.2730 - val_loss: 1.0786 - val_accuracy: 0.3230\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 1.0785 - accuracy: 0.2941 - val_loss: 1.0784 - val_accuracy: 0.3410\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 1.0783 - accuracy: 0.3099 - val_loss: 1.0781 - val_accuracy: 0.3537\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0781 - accuracy: 0.3197 - val_loss: 1.0778 - val_accuracy: 0.3638\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0778 - accuracy: 0.3303 - val_loss: 1.0775 - val_accuracy: 0.3665\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 1.0775 - accuracy: 0.3647\n",
            "first layer: 128 | second layer : 175, third layer 225\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.0826 - accuracy: 0.8589 - val_loss: 0.0262 - val_accuracy: 0.9592\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0289 - accuracy: 0.9539 - val_loss: 0.0180 - val_accuracy: 0.9717\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0198 - accuracy: 0.9688 - val_loss: 0.0158 - val_accuracy: 0.9743\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0151 - accuracy: 0.9763 - val_loss: 0.0139 - val_accuracy: 0.9773\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0119 - accuracy: 0.9812 - val_loss: 0.0138 - val_accuracy: 0.9783\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0096 - accuracy: 0.9841 - val_loss: 0.0132 - val_accuracy: 0.9785\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0073 - accuracy: 0.9887 - val_loss: 0.0131 - val_accuracy: 0.9787\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0063 - accuracy: 0.9901 - val_loss: 0.0130 - val_accuracy: 0.9790\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0053 - accuracy: 0.9915 - val_loss: 0.0132 - val_accuracy: 0.9807\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0047 - accuracy: 0.9928 - val_loss: 0.0128 - val_accuracy: 0.9817\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0131 - accuracy: 0.9822\n",
            "first layer: 128 | second layer : 200, third layer 175\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 1.0800 - accuracy: 0.0801 - val_loss: 1.0799 - val_accuracy: 0.0847\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 7s 16ms/step - loss: 1.0798 - accuracy: 0.0875 - val_loss: 1.0798 - val_accuracy: 0.0858\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 7s 16ms/step - loss: 1.0797 - accuracy: 0.0926 - val_loss: 1.0796 - val_accuracy: 0.0880\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 5s 11ms/step - loss: 1.0795 - accuracy: 0.1000 - val_loss: 1.0794 - val_accuracy: 0.0927\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 1.0794 - accuracy: 0.1085 - val_loss: 1.0793 - val_accuracy: 0.1013\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0792 - accuracy: 0.1196 - val_loss: 1.0791 - val_accuracy: 0.1122\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0790 - accuracy: 0.1342 - val_loss: 1.0789 - val_accuracy: 0.1272\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0788 - accuracy: 0.1508 - val_loss: 1.0787 - val_accuracy: 0.1477\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0786 - accuracy: 0.1658 - val_loss: 1.0784 - val_accuracy: 0.1720\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 1.0784 - accuracy: 0.1828 - val_loss: 1.0782 - val_accuracy: 0.1932\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 1.0781 - accuracy: 0.1971\n",
            "first layer: 128 | second layer : 200, third layer 200\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.0825 - accuracy: 0.8611 - val_loss: 0.0250 - val_accuracy: 0.9618\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.0283 - accuracy: 0.9539 - val_loss: 0.0177 - val_accuracy: 0.9717\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 6s 15ms/step - loss: 0.0191 - accuracy: 0.9693 - val_loss: 0.0161 - val_accuracy: 0.9748\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.0147 - accuracy: 0.9763 - val_loss: 0.0133 - val_accuracy: 0.9782\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0113 - accuracy: 0.9824 - val_loss: 0.0149 - val_accuracy: 0.9757\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0091 - accuracy: 0.9856 - val_loss: 0.0140 - val_accuracy: 0.9777\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0076 - accuracy: 0.9879 - val_loss: 0.0130 - val_accuracy: 0.9805\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.0059 - accuracy: 0.9905 - val_loss: 0.0143 - val_accuracy: 0.9780\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.0050 - accuracy: 0.9924 - val_loss: 0.0137 - val_accuracy: 0.9810\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.0044 - accuracy: 0.9929 - val_loss: 0.0150 - val_accuracy: 0.9768\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0151 - accuracy: 0.9792\n",
            "first layer: 128 | second layer : 200, third layer 225\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 12s 8ms/step - loss: 1.0801 - accuracy: 0.0815 - val_loss: 1.0800 - val_accuracy: 0.0945\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0799 - accuracy: 0.0937 - val_loss: 1.0798 - val_accuracy: 0.1062\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 5s 11ms/step - loss: 1.0798 - accuracy: 0.1057 - val_loss: 1.0797 - val_accuracy: 0.1178\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 1.0796 - accuracy: 0.1182 - val_loss: 1.0795 - val_accuracy: 0.1328\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0795 - accuracy: 0.1322 - val_loss: 1.0793 - val_accuracy: 0.1505\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0793 - accuracy: 0.1509 - val_loss: 1.0791 - val_accuracy: 0.1700\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0791 - accuracy: 0.1665 - val_loss: 1.0789 - val_accuracy: 0.1930\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0789 - accuracy: 0.1838 - val_loss: 1.0787 - val_accuracy: 0.2168\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0787 - accuracy: 0.1995 - val_loss: 1.0785 - val_accuracy: 0.2333\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 1.0784 - accuracy: 0.2126 - val_loss: 1.0782 - val_accuracy: 0.2472\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 1.0782 - accuracy: 0.2286\n",
            "first layer: 128 | second layer : 225, third layer 175\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.0812 - accuracy: 0.8647 - val_loss: 0.0252 - val_accuracy: 0.9605\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0264 - accuracy: 0.9572 - val_loss: 0.0162 - val_accuracy: 0.9757\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0177 - accuracy: 0.9717 - val_loss: 0.0142 - val_accuracy: 0.9765\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0135 - accuracy: 0.9784 - val_loss: 0.0125 - val_accuracy: 0.9803\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0104 - accuracy: 0.9833 - val_loss: 0.0130 - val_accuracy: 0.9780\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.0082 - accuracy: 0.9875 - val_loss: 0.0126 - val_accuracy: 0.9782\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 5s 12ms/step - loss: 0.0069 - accuracy: 0.9895 - val_loss: 0.0122 - val_accuracy: 0.9805\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0053 - accuracy: 0.9919 - val_loss: 0.0124 - val_accuracy: 0.9807\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0045 - accuracy: 0.9932 - val_loss: 0.0135 - val_accuracy: 0.9815\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0039 - accuracy: 0.9940 - val_loss: 0.0133 - val_accuracy: 0.9820\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0147 - accuracy: 0.9804\n",
            "first layer: 128 | second layer : 225, third layer 200\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 1.0801 - accuracy: 0.0907 - val_loss: 1.0800 - val_accuracy: 0.0977\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0800 - accuracy: 0.1036 - val_loss: 1.0799 - val_accuracy: 0.1132\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0799 - accuracy: 0.1161 - val_loss: 1.0798 - val_accuracy: 0.1260\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0798 - accuracy: 0.1251 - val_loss: 1.0797 - val_accuracy: 0.1447\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 6s 14ms/step - loss: 1.0796 - accuracy: 0.1402 - val_loss: 1.0795 - val_accuracy: 0.1602\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 6s 14ms/step - loss: 1.0795 - accuracy: 0.1528 - val_loss: 1.0794 - val_accuracy: 0.1727\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 5s 11ms/step - loss: 1.0794 - accuracy: 0.1645 - val_loss: 1.0792 - val_accuracy: 0.1875\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0792 - accuracy: 0.1760 - val_loss: 1.0791 - val_accuracy: 0.2008\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0791 - accuracy: 0.1882 - val_loss: 1.0789 - val_accuracy: 0.2103\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 1.0789 - accuracy: 0.1981 - val_loss: 1.0787 - val_accuracy: 0.2213\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 1.0788 - accuracy: 0.2109\n",
            "first layer: 128 | second layer : 225, third layer 225\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.0797 - accuracy: 0.8659 - val_loss: 0.0258 - val_accuracy: 0.9600\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0284 - accuracy: 0.9542 - val_loss: 0.0181 - val_accuracy: 0.9722\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0195 - accuracy: 0.9697 - val_loss: 0.0147 - val_accuracy: 0.9777\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0147 - accuracy: 0.9760 - val_loss: 0.0135 - val_accuracy: 0.9785\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0117 - accuracy: 0.9816 - val_loss: 0.0134 - val_accuracy: 0.9807\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0092 - accuracy: 0.9854 - val_loss: 0.0121 - val_accuracy: 0.9817\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0076 - accuracy: 0.9877 - val_loss: 0.0127 - val_accuracy: 0.9805\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0060 - accuracy: 0.9907 - val_loss: 0.0130 - val_accuracy: 0.9810\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0050 - accuracy: 0.9926 - val_loss: 0.0137 - val_accuracy: 0.9783\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 6s 13ms/step - loss: 0.0045 - accuracy: 0.9927 - val_loss: 0.0130 - val_accuracy: 0.9805\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0132 - accuracy: 0.9810\n"
          ]
        }
      ],
      "source": [
        "if device_name != '/device:GPU:0':  \n",
        "  model_builder2()\n",
        "else:\n",
        "    with tf.device('GPU:0'):\n",
        "        model_builder2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first layer: 128 | second layer 190 | third layer 190\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 5s 8ms/step - loss: 0.5573 - accuracy: 0.7725 - val_loss: 0.5130 - val_accuracy: 0.9350\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.5142 - accuracy: 0.9297 - val_loss: 0.5019 - val_accuracy: 0.9573\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.5047 - accuracy: 0.9485 - val_loss: 0.4975 - val_accuracy: 0.9615\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.4994 - accuracy: 0.9586 - val_loss: 0.4937 - val_accuracy: 0.9690\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.4960 - accuracy: 0.9647 - val_loss: 0.4923 - val_accuracy: 0.9703\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.4936 - accuracy: 0.9694 - val_loss: 0.4920 - val_accuracy: 0.9700\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.4916 - accuracy: 0.9732 - val_loss: 0.4905 - val_accuracy: 0.9745\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.4901 - accuracy: 0.9761 - val_loss: 0.4902 - val_accuracy: 0.9742\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.4889 - accuracy: 0.9791 - val_loss: 0.4897 - val_accuracy: 0.9768\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.4879 - accuracy: 0.9812 - val_loss: 0.4890 - val_accuracy: 0.9773\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.4907 - accuracy: 0.9770\n",
            "first layer: 128 | second layer 190 | third layer 200\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 6s 10ms/step - loss: 0.7219 - accuracy: 0.7254 - val_loss: 0.6944 - val_accuracy: 0.8840\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 5s 13ms/step - loss: 0.6938 - accuracy: 0.9128 - val_loss: 0.6833 - val_accuracy: 0.9537\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 6s 15ms/step - loss: 0.6867 - accuracy: 0.9412 - val_loss: 0.6797 - val_accuracy: 0.9593\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6830 - accuracy: 0.9528 - val_loss: 0.6788 - val_accuracy: 0.9655\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6807 - accuracy: 0.9599 - val_loss: 0.6783 - val_accuracy: 0.9682\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6793 - accuracy: 0.9657 - val_loss: 0.6770 - val_accuracy: 0.9707\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6781 - accuracy: 0.9696 - val_loss: 0.6770 - val_accuracy: 0.9690\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6772 - accuracy: 0.9722 - val_loss: 0.6762 - val_accuracy: 0.9753\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6764 - accuracy: 0.9751 - val_loss: 0.6754 - val_accuracy: 0.9758\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6758 - accuracy: 0.9778 - val_loss: 0.6759 - val_accuracy: 0.9760\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.6769 - accuracy: 0.9748\n",
            "first layer: 128 | second layer 190 | third layer 210\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.5595 - accuracy: 0.7785 - val_loss: 0.5209 - val_accuracy: 0.9318\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.5229 - accuracy: 0.9219 - val_loss: 0.5121 - val_accuracy: 0.9457\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.5137 - accuracy: 0.9435 - val_loss: 0.5063 - val_accuracy: 0.9613\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.5088 - accuracy: 0.9539 - val_loss: 0.5028 - val_accuracy: 0.9638\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.5053 - accuracy: 0.9612 - val_loss: 0.5015 - val_accuracy: 0.9667\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.5031 - accuracy: 0.9651 - val_loss: 0.5005 - val_accuracy: 0.9710\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.5009 - accuracy: 0.9696 - val_loss: 0.5004 - val_accuracy: 0.9728\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.4995 - accuracy: 0.9731 - val_loss: 0.4988 - val_accuracy: 0.9765\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.4984 - accuracy: 0.9754 - val_loss: 0.4986 - val_accuracy: 0.9747\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.4976 - accuracy: 0.9773 - val_loss: 0.4974 - val_accuracy: 0.9783\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.4997 - accuracy: 0.9746\n",
            "first layer: 128 | second layer 200 | third layer 190\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.7332 - accuracy: 0.6819 - val_loss: 0.7111 - val_accuracy: 0.9007\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.7094 - accuracy: 0.9083 - val_loss: 0.7019 - val_accuracy: 0.9453\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.7037 - accuracy: 0.9337 - val_loss: 0.6994 - val_accuracy: 0.9550\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.7007 - accuracy: 0.9469 - val_loss: 0.6971 - val_accuracy: 0.9667\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6987 - accuracy: 0.9564 - val_loss: 0.6970 - val_accuracy: 0.9653\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6975 - accuracy: 0.9618 - val_loss: 0.6959 - val_accuracy: 0.9707\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6965 - accuracy: 0.9665 - val_loss: 0.6953 - val_accuracy: 0.9727\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6957 - accuracy: 0.9694 - val_loss: 0.6960 - val_accuracy: 0.9700\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6950 - accuracy: 0.9717 - val_loss: 0.6954 - val_accuracy: 0.9725\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6946 - accuracy: 0.9744 - val_loss: 0.6947 - val_accuracy: 0.9768\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.6955 - accuracy: 0.9738\n",
            "first layer: 128 | second layer 200 | third layer 200\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.5611 - accuracy: 0.7661 - val_loss: 0.5207 - val_accuracy: 0.9330\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.5217 - accuracy: 0.9259 - val_loss: 0.5103 - val_accuracy: 0.9585\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.5131 - accuracy: 0.9451 - val_loss: 0.5060 - val_accuracy: 0.9623\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.5082 - accuracy: 0.9559 - val_loss: 0.5034 - val_accuracy: 0.9677\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.5046 - accuracy: 0.9635 - val_loss: 0.5010 - val_accuracy: 0.9738\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.5026 - accuracy: 0.9676 - val_loss: 0.4999 - val_accuracy: 0.9733\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 6ms/step - loss: 0.5008 - accuracy: 0.9718 - val_loss: 0.4994 - val_accuracy: 0.9758\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.4997 - accuracy: 0.9740 - val_loss: 0.4989 - val_accuracy: 0.9763\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.4984 - accuracy: 0.9772 - val_loss: 0.4988 - val_accuracy: 0.9782\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.4975 - accuracy: 0.9789 - val_loss: 0.4995 - val_accuracy: 0.9780\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.5002 - accuracy: 0.9732\n",
            "first layer: 128 | second layer 200 | third layer 210\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 5s 10ms/step - loss: 0.7313 - accuracy: 0.6356 - val_loss: 0.7101 - val_accuracy: 0.8283\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.7098 - accuracy: 0.8833 - val_loss: 0.7009 - val_accuracy: 0.9403\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.7033 - accuracy: 0.9308 - val_loss: 0.6986 - val_accuracy: 0.9505\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.7002 - accuracy: 0.9455 - val_loss: 0.6962 - val_accuracy: 0.9643\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6982 - accuracy: 0.9531 - val_loss: 0.6957 - val_accuracy: 0.9673\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6969 - accuracy: 0.9601 - val_loss: 0.6947 - val_accuracy: 0.9687\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6958 - accuracy: 0.9653 - val_loss: 0.6948 - val_accuracy: 0.9723\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6952 - accuracy: 0.9677 - val_loss: 0.6942 - val_accuracy: 0.9730\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6945 - accuracy: 0.9709 - val_loss: 0.6938 - val_accuracy: 0.9738\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6940 - accuracy: 0.9739 - val_loss: 0.6940 - val_accuracy: 0.9757\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.6944 - accuracy: 0.9738\n",
            "first layer: 128 | second layer 210 | third layer 190\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.5686 - accuracy: 0.7218 - val_loss: 0.5300 - val_accuracy: 0.9282\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.5304 - accuracy: 0.9214 - val_loss: 0.5200 - val_accuracy: 0.9502\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.5214 - accuracy: 0.9414 - val_loss: 0.5151 - val_accuracy: 0.9585\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.5168 - accuracy: 0.9523 - val_loss: 0.5130 - val_accuracy: 0.9622\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.5139 - accuracy: 0.9606 - val_loss: 0.5108 - val_accuracy: 0.9673\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.5115 - accuracy: 0.9652 - val_loss: 0.5092 - val_accuracy: 0.9710\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.5098 - accuracy: 0.9701 - val_loss: 0.5091 - val_accuracy: 0.9727\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.5086 - accuracy: 0.9716 - val_loss: 0.5084 - val_accuracy: 0.9733\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.5077 - accuracy: 0.9754 - val_loss: 0.5082 - val_accuracy: 0.9745\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.5066 - accuracy: 0.9777 - val_loss: 0.5075 - val_accuracy: 0.9775\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.5097 - accuracy: 0.9720\n",
            "first layer: 128 | second layer 210 | third layer 200\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 5s 10ms/step - loss: 0.7209 - accuracy: 0.7447 - val_loss: 0.6944 - val_accuracy: 0.8892\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6930 - accuracy: 0.9181 - val_loss: 0.6833 - val_accuracy: 0.9532\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6863 - accuracy: 0.9417 - val_loss: 0.6800 - val_accuracy: 0.9585\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6828 - accuracy: 0.9532 - val_loss: 0.6784 - val_accuracy: 0.9673\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6806 - accuracy: 0.9608 - val_loss: 0.6774 - val_accuracy: 0.9708\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6791 - accuracy: 0.9657 - val_loss: 0.6770 - val_accuracy: 0.9742\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6780 - accuracy: 0.9702 - val_loss: 0.6768 - val_accuracy: 0.9692\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6772 - accuracy: 0.9726 - val_loss: 0.6766 - val_accuracy: 0.9743\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.6763 - accuracy: 0.9757 - val_loss: 0.6764 - val_accuracy: 0.9727\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 4s 10ms/step - loss: 0.6757 - accuracy: 0.9778 - val_loss: 0.6760 - val_accuracy: 0.9762\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.6769 - accuracy: 0.9728\n",
            "first layer: 128 | second layer 210 | third layer 210\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.5676 - accuracy: 0.7271 - val_loss: 0.5305 - val_accuracy: 0.9172\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.5304 - accuracy: 0.9136 - val_loss: 0.5195 - val_accuracy: 0.9532\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.5212 - accuracy: 0.9405 - val_loss: 0.5157 - val_accuracy: 0.9540\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.5163 - accuracy: 0.9508 - val_loss: 0.5120 - val_accuracy: 0.9632\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.5131 - accuracy: 0.9597 - val_loss: 0.5096 - val_accuracy: 0.9710\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.5108 - accuracy: 0.9650 - val_loss: 0.5092 - val_accuracy: 0.9720\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.5092 - accuracy: 0.9700 - val_loss: 0.5086 - val_accuracy: 0.9732\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.5076 - accuracy: 0.9734 - val_loss: 0.5079 - val_accuracy: 0.9753\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.5067 - accuracy: 0.9757 - val_loss: 0.5078 - val_accuracy: 0.9768\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 3s 7ms/step - loss: 0.5058 - accuracy: 0.9779 - val_loss: 0.5081 - val_accuracy: 0.9750\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.5087 - accuracy: 0.9728\n"
          ]
        }
      ],
      "source": [
        "if device_name != '/device:GPU:0':  \n",
        "  model_builder3()\n",
        "else:\n",
        "    with tf.device('GPU:0'):\n",
        "        model_builder3()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HisIR0-mqim-"
      },
      "source": [
        "### Creating the dataframe that denotes all hyperparameters and their respective loss and accuracy ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Vw218uDmqhtx"
      },
      "outputs": [],
      "source": [
        "model_data = {\n",
        "    'model_names' : model_names,\n",
        "    'first_layer_nodes' : first_layer_nodes,\n",
        "    'first_layer_activation' : first_layer_activation,\n",
        "    'second_layer_nodes' : second_layer_nodes,\n",
        "    'second_layer_activation' : second_layer_activation,\n",
        "    'third_layer_nodes' : third_layer_nodes,\n",
        "    'third_layer_activation' : third_layer_activation,\n",
        "    'output_layer_activation' : output_layer_activation,\n",
        "    'optimizer_function' : optimizer_function,\n",
        "    'loss_function' : loss_function,\n",
        "    'batch_size' : batch_size,\n",
        "    'num_of_epochs' : num_of_epochs,\n",
        "    'training_loss' : training_loss,\n",
        "    'training_accuracy' : training_accuracy,\n",
        "    'validation_loss' : validation_loss,\n",
        "    'validation_accuracy' : validation_accuracy,\n",
        "    'test_loss' : test_loss,\n",
        "    'test_accuracy' : test_accuracy\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eYzgm1Lq1yq"
      },
      "source": [
        "**Dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "O78b_Gw-qv_c"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(data = model_data)\n",
        "df = df.set_index('model_names')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Full dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>first_layer_nodes</th>\n",
              "      <th>first_layer_activation</th>\n",
              "      <th>second_layer_nodes</th>\n",
              "      <th>second_layer_activation</th>\n",
              "      <th>third_layer_nodes</th>\n",
              "      <th>third_layer_activation</th>\n",
              "      <th>output_layer_activation</th>\n",
              "      <th>optimizer_function</th>\n",
              "      <th>loss_function</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>num_of_epochs</th>\n",
              "      <th>training_loss</th>\n",
              "      <th>training_accuracy</th>\n",
              "      <th>validation_loss</th>\n",
              "      <th>validation_accuracy</th>\n",
              "      <th>test_loss</th>\n",
              "      <th>test_accuracy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model_names</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>NN_Model1</th>\n",
              "      <td>128</td>\n",
              "      <td>relu</td>\n",
              "      <td>150</td>\n",
              "      <td>relu</td>\n",
              "      <td>150</td>\n",
              "      <td>relu</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>adam</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.004854</td>\n",
              "      <td>0.992315</td>\n",
              "      <td>0.011905</td>\n",
              "      <td>0.980000</td>\n",
              "      <td>0.016297</td>\n",
              "      <td>0.9764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model2</th>\n",
              "      <td>128</td>\n",
              "      <td>relu</td>\n",
              "      <td>150</td>\n",
              "      <td>relu</td>\n",
              "      <td>200</td>\n",
              "      <td>relu</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>adam</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.004738</td>\n",
              "      <td>0.992463</td>\n",
              "      <td>0.012179</td>\n",
              "      <td>0.981667</td>\n",
              "      <td>0.015989</td>\n",
              "      <td>0.9775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model3</th>\n",
              "      <td>128</td>\n",
              "      <td>relu</td>\n",
              "      <td>150</td>\n",
              "      <td>relu</td>\n",
              "      <td>250</td>\n",
              "      <td>relu</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>adam</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.004985</td>\n",
              "      <td>0.992019</td>\n",
              "      <td>0.012793</td>\n",
              "      <td>0.981167</td>\n",
              "      <td>0.014604</td>\n",
              "      <td>0.9787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model4</th>\n",
              "      <td>128</td>\n",
              "      <td>relu</td>\n",
              "      <td>200</td>\n",
              "      <td>relu</td>\n",
              "      <td>150</td>\n",
              "      <td>relu</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>adam</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.004483</td>\n",
              "      <td>0.992889</td>\n",
              "      <td>0.012221</td>\n",
              "      <td>0.981333</td>\n",
              "      <td>0.015238</td>\n",
              "      <td>0.9804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model5</th>\n",
              "      <td>128</td>\n",
              "      <td>relu</td>\n",
              "      <td>200</td>\n",
              "      <td>relu</td>\n",
              "      <td>200</td>\n",
              "      <td>relu</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>adam</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.004479</td>\n",
              "      <td>0.993185</td>\n",
              "      <td>0.012013</td>\n",
              "      <td>0.981667</td>\n",
              "      <td>0.014029</td>\n",
              "      <td>0.9797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model6</th>\n",
              "      <td>128</td>\n",
              "      <td>relu</td>\n",
              "      <td>200</td>\n",
              "      <td>relu</td>\n",
              "      <td>250</td>\n",
              "      <td>relu</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>adam</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.004363</td>\n",
              "      <td>0.993185</td>\n",
              "      <td>0.012088</td>\n",
              "      <td>0.981667</td>\n",
              "      <td>0.015033</td>\n",
              "      <td>0.9804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model7</th>\n",
              "      <td>128</td>\n",
              "      <td>relu</td>\n",
              "      <td>250</td>\n",
              "      <td>relu</td>\n",
              "      <td>150</td>\n",
              "      <td>relu</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>adam</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.004343</td>\n",
              "      <td>0.992741</td>\n",
              "      <td>0.012231</td>\n",
              "      <td>0.981000</td>\n",
              "      <td>0.016622</td>\n",
              "      <td>0.9771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model8</th>\n",
              "      <td>128</td>\n",
              "      <td>relu</td>\n",
              "      <td>250</td>\n",
              "      <td>relu</td>\n",
              "      <td>200</td>\n",
              "      <td>relu</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>adam</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.003791</td>\n",
              "      <td>0.994241</td>\n",
              "      <td>0.012957</td>\n",
              "      <td>0.980000</td>\n",
              "      <td>0.015725</td>\n",
              "      <td>0.9788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model9</th>\n",
              "      <td>128</td>\n",
              "      <td>relu</td>\n",
              "      <td>250</td>\n",
              "      <td>relu</td>\n",
              "      <td>250</td>\n",
              "      <td>relu</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>adam</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.004615</td>\n",
              "      <td>0.992463</td>\n",
              "      <td>0.013242</td>\n",
              "      <td>0.983167</td>\n",
              "      <td>0.015259</td>\n",
              "      <td>0.9778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model10</th>\n",
              "      <td>128</td>\n",
              "      <td>gelu</td>\n",
              "      <td>175</td>\n",
              "      <td>gelu</td>\n",
              "      <td>175</td>\n",
              "      <td>gelu</td>\n",
              "      <td>softmax</td>\n",
              "      <td>adam</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.004314</td>\n",
              "      <td>0.993315</td>\n",
              "      <td>0.012950</td>\n",
              "      <td>0.980167</td>\n",
              "      <td>0.016143</td>\n",
              "      <td>0.9765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model11</th>\n",
              "      <td>128</td>\n",
              "      <td>gelu</td>\n",
              "      <td>175</td>\n",
              "      <td>gelu</td>\n",
              "      <td>200</td>\n",
              "      <td>gelu</td>\n",
              "      <td>softmax</td>\n",
              "      <td>SGD</td>\n",
              "      <td>hinge</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>1.077790</td>\n",
              "      <td>0.330278</td>\n",
              "      <td>1.077535</td>\n",
              "      <td>0.366500</td>\n",
              "      <td>1.077523</td>\n",
              "      <td>0.3647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model12</th>\n",
              "      <td>128</td>\n",
              "      <td>gelu</td>\n",
              "      <td>175</td>\n",
              "      <td>gelu</td>\n",
              "      <td>225</td>\n",
              "      <td>gelu</td>\n",
              "      <td>softmax</td>\n",
              "      <td>adam</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.004667</td>\n",
              "      <td>0.992778</td>\n",
              "      <td>0.012754</td>\n",
              "      <td>0.981667</td>\n",
              "      <td>0.013101</td>\n",
              "      <td>0.9822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model13</th>\n",
              "      <td>128</td>\n",
              "      <td>gelu</td>\n",
              "      <td>200</td>\n",
              "      <td>gelu</td>\n",
              "      <td>175</td>\n",
              "      <td>gelu</td>\n",
              "      <td>softmax</td>\n",
              "      <td>SGD</td>\n",
              "      <td>hinge</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>1.078355</td>\n",
              "      <td>0.182759</td>\n",
              "      <td>1.078171</td>\n",
              "      <td>0.193167</td>\n",
              "      <td>1.078138</td>\n",
              "      <td>0.1971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model14</th>\n",
              "      <td>128</td>\n",
              "      <td>gelu</td>\n",
              "      <td>200</td>\n",
              "      <td>gelu</td>\n",
              "      <td>200</td>\n",
              "      <td>gelu</td>\n",
              "      <td>softmax</td>\n",
              "      <td>adam</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.004430</td>\n",
              "      <td>0.992870</td>\n",
              "      <td>0.013016</td>\n",
              "      <td>0.981000</td>\n",
              "      <td>0.015119</td>\n",
              "      <td>0.9792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model15</th>\n",
              "      <td>128</td>\n",
              "      <td>gelu</td>\n",
              "      <td>200</td>\n",
              "      <td>gelu</td>\n",
              "      <td>225</td>\n",
              "      <td>gelu</td>\n",
              "      <td>softmax</td>\n",
              "      <td>SGD</td>\n",
              "      <td>hinge</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>1.078435</td>\n",
              "      <td>0.212630</td>\n",
              "      <td>1.078178</td>\n",
              "      <td>0.247167</td>\n",
              "      <td>1.078247</td>\n",
              "      <td>0.2286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model16</th>\n",
              "      <td>128</td>\n",
              "      <td>gelu</td>\n",
              "      <td>225</td>\n",
              "      <td>gelu</td>\n",
              "      <td>175</td>\n",
              "      <td>gelu</td>\n",
              "      <td>softmax</td>\n",
              "      <td>adam</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.003907</td>\n",
              "      <td>0.994000</td>\n",
              "      <td>0.012220</td>\n",
              "      <td>0.982000</td>\n",
              "      <td>0.014652</td>\n",
              "      <td>0.9804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model17</th>\n",
              "      <td>128</td>\n",
              "      <td>gelu</td>\n",
              "      <td>225</td>\n",
              "      <td>gelu</td>\n",
              "      <td>200</td>\n",
              "      <td>gelu</td>\n",
              "      <td>softmax</td>\n",
              "      <td>SGD</td>\n",
              "      <td>hinge</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>1.078915</td>\n",
              "      <td>0.198056</td>\n",
              "      <td>1.078746</td>\n",
              "      <td>0.221333</td>\n",
              "      <td>1.078786</td>\n",
              "      <td>0.2109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model18</th>\n",
              "      <td>128</td>\n",
              "      <td>gelu</td>\n",
              "      <td>225</td>\n",
              "      <td>gelu</td>\n",
              "      <td>225</td>\n",
              "      <td>gelu</td>\n",
              "      <td>softmax</td>\n",
              "      <td>adam</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.004502</td>\n",
              "      <td>0.992741</td>\n",
              "      <td>0.012079</td>\n",
              "      <td>0.981667</td>\n",
              "      <td>0.013233</td>\n",
              "      <td>0.9810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model19</th>\n",
              "      <td>128</td>\n",
              "      <td>elu</td>\n",
              "      <td>190</td>\n",
              "      <td>elu</td>\n",
              "      <td>190</td>\n",
              "      <td>elu</td>\n",
              "      <td>swish</td>\n",
              "      <td>adam</td>\n",
              "      <td>squared_hinge</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.487915</td>\n",
              "      <td>0.981185</td>\n",
              "      <td>0.489006</td>\n",
              "      <td>0.977333</td>\n",
              "      <td>0.490673</td>\n",
              "      <td>0.9770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model20</th>\n",
              "      <td>128</td>\n",
              "      <td>elu</td>\n",
              "      <td>190</td>\n",
              "      <td>elu</td>\n",
              "      <td>200</td>\n",
              "      <td>elu</td>\n",
              "      <td>swish</td>\n",
              "      <td>RMSprop</td>\n",
              "      <td>hinge</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.675777</td>\n",
              "      <td>0.977759</td>\n",
              "      <td>0.675434</td>\n",
              "      <td>0.976000</td>\n",
              "      <td>0.676935</td>\n",
              "      <td>0.9748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model21</th>\n",
              "      <td>128</td>\n",
              "      <td>elu</td>\n",
              "      <td>190</td>\n",
              "      <td>elu</td>\n",
              "      <td>210</td>\n",
              "      <td>elu</td>\n",
              "      <td>swish</td>\n",
              "      <td>adam</td>\n",
              "      <td>squared_hinge</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.497580</td>\n",
              "      <td>0.977315</td>\n",
              "      <td>0.497425</td>\n",
              "      <td>0.978333</td>\n",
              "      <td>0.499706</td>\n",
              "      <td>0.9746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model22</th>\n",
              "      <td>128</td>\n",
              "      <td>elu</td>\n",
              "      <td>200</td>\n",
              "      <td>elu</td>\n",
              "      <td>190</td>\n",
              "      <td>elu</td>\n",
              "      <td>swish</td>\n",
              "      <td>RMSprop</td>\n",
              "      <td>hinge</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.694573</td>\n",
              "      <td>0.974407</td>\n",
              "      <td>0.694674</td>\n",
              "      <td>0.976833</td>\n",
              "      <td>0.695544</td>\n",
              "      <td>0.9738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model23</th>\n",
              "      <td>128</td>\n",
              "      <td>elu</td>\n",
              "      <td>200</td>\n",
              "      <td>elu</td>\n",
              "      <td>200</td>\n",
              "      <td>elu</td>\n",
              "      <td>swish</td>\n",
              "      <td>adam</td>\n",
              "      <td>squared_hinge</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.978870</td>\n",
              "      <td>0.498778</td>\n",
              "      <td>0.978167</td>\n",
              "      <td>0.500178</td>\n",
              "      <td>0.9732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model24</th>\n",
              "      <td>128</td>\n",
              "      <td>elu</td>\n",
              "      <td>200</td>\n",
              "      <td>elu</td>\n",
              "      <td>210</td>\n",
              "      <td>elu</td>\n",
              "      <td>swish</td>\n",
              "      <td>RMSprop</td>\n",
              "      <td>hinge</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.693955</td>\n",
              "      <td>0.973852</td>\n",
              "      <td>0.693781</td>\n",
              "      <td>0.975667</td>\n",
              "      <td>0.694413</td>\n",
              "      <td>0.9738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model25</th>\n",
              "      <td>128</td>\n",
              "      <td>elu</td>\n",
              "      <td>210</td>\n",
              "      <td>elu</td>\n",
              "      <td>190</td>\n",
              "      <td>elu</td>\n",
              "      <td>swish</td>\n",
              "      <td>adam</td>\n",
              "      <td>squared_hinge</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.506632</td>\n",
              "      <td>0.977704</td>\n",
              "      <td>0.507458</td>\n",
              "      <td>0.977500</td>\n",
              "      <td>0.509736</td>\n",
              "      <td>0.9720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model26</th>\n",
              "      <td>128</td>\n",
              "      <td>elu</td>\n",
              "      <td>210</td>\n",
              "      <td>elu</td>\n",
              "      <td>200</td>\n",
              "      <td>elu</td>\n",
              "      <td>swish</td>\n",
              "      <td>RMSprop</td>\n",
              "      <td>hinge</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.675679</td>\n",
              "      <td>0.977778</td>\n",
              "      <td>0.675963</td>\n",
              "      <td>0.976167</td>\n",
              "      <td>0.676876</td>\n",
              "      <td>0.9728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NN_Model27</th>\n",
              "      <td>128</td>\n",
              "      <td>elu</td>\n",
              "      <td>210</td>\n",
              "      <td>elu</td>\n",
              "      <td>210</td>\n",
              "      <td>elu</td>\n",
              "      <td>swish</td>\n",
              "      <td>adam</td>\n",
              "      <td>squared_hinge</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.505768</td>\n",
              "      <td>0.977944</td>\n",
              "      <td>0.507822</td>\n",
              "      <td>0.976833</td>\n",
              "      <td>0.508668</td>\n",
              "      <td>0.9728</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             first_layer_nodes first_layer_activation  second_layer_nodes  \\\n",
              "model_names                                                                 \n",
              "NN_Model1                  128                   relu                 150   \n",
              "NN_Model2                  128                   relu                 150   \n",
              "NN_Model3                  128                   relu                 150   \n",
              "NN_Model4                  128                   relu                 200   \n",
              "NN_Model5                  128                   relu                 200   \n",
              "NN_Model6                  128                   relu                 200   \n",
              "NN_Model7                  128                   relu                 250   \n",
              "NN_Model8                  128                   relu                 250   \n",
              "NN_Model9                  128                   relu                 250   \n",
              "NN_Model10                 128                   gelu                 175   \n",
              "NN_Model11                 128                   gelu                 175   \n",
              "NN_Model12                 128                   gelu                 175   \n",
              "NN_Model13                 128                   gelu                 200   \n",
              "NN_Model14                 128                   gelu                 200   \n",
              "NN_Model15                 128                   gelu                 200   \n",
              "NN_Model16                 128                   gelu                 225   \n",
              "NN_Model17                 128                   gelu                 225   \n",
              "NN_Model18                 128                   gelu                 225   \n",
              "NN_Model19                 128                    elu                 190   \n",
              "NN_Model20                 128                    elu                 190   \n",
              "NN_Model21                 128                    elu                 190   \n",
              "NN_Model22                 128                    elu                 200   \n",
              "NN_Model23                 128                    elu                 200   \n",
              "NN_Model24                 128                    elu                 200   \n",
              "NN_Model25                 128                    elu                 210   \n",
              "NN_Model26                 128                    elu                 210   \n",
              "NN_Model27                 128                    elu                 210   \n",
              "\n",
              "            second_layer_activation  third_layer_nodes third_layer_activation  \\\n",
              "model_names                                                                     \n",
              "NN_Model1                      relu                150                   relu   \n",
              "NN_Model2                      relu                200                   relu   \n",
              "NN_Model3                      relu                250                   relu   \n",
              "NN_Model4                      relu                150                   relu   \n",
              "NN_Model5                      relu                200                   relu   \n",
              "NN_Model6                      relu                250                   relu   \n",
              "NN_Model7                      relu                150                   relu   \n",
              "NN_Model8                      relu                200                   relu   \n",
              "NN_Model9                      relu                250                   relu   \n",
              "NN_Model10                     gelu                175                   gelu   \n",
              "NN_Model11                     gelu                200                   gelu   \n",
              "NN_Model12                     gelu                225                   gelu   \n",
              "NN_Model13                     gelu                175                   gelu   \n",
              "NN_Model14                     gelu                200                   gelu   \n",
              "NN_Model15                     gelu                225                   gelu   \n",
              "NN_Model16                     gelu                175                   gelu   \n",
              "NN_Model17                     gelu                200                   gelu   \n",
              "NN_Model18                     gelu                225                   gelu   \n",
              "NN_Model19                      elu                190                    elu   \n",
              "NN_Model20                      elu                200                    elu   \n",
              "NN_Model21                      elu                210                    elu   \n",
              "NN_Model22                      elu                190                    elu   \n",
              "NN_Model23                      elu                200                    elu   \n",
              "NN_Model24                      elu                210                    elu   \n",
              "NN_Model25                      elu                190                    elu   \n",
              "NN_Model26                      elu                200                    elu   \n",
              "NN_Model27                      elu                210                    elu   \n",
              "\n",
              "            output_layer_activation optimizer_function        loss_function  \\\n",
              "model_names                                                                   \n",
              "NN_Model1                   sigmoid               adam  binary_crossentropy   \n",
              "NN_Model2                   sigmoid               adam  binary_crossentropy   \n",
              "NN_Model3                   sigmoid               adam  binary_crossentropy   \n",
              "NN_Model4                   sigmoid               adam  binary_crossentropy   \n",
              "NN_Model5                   sigmoid               adam  binary_crossentropy   \n",
              "NN_Model6                   sigmoid               adam  binary_crossentropy   \n",
              "NN_Model7                   sigmoid               adam  binary_crossentropy   \n",
              "NN_Model8                   sigmoid               adam  binary_crossentropy   \n",
              "NN_Model9                   sigmoid               adam  binary_crossentropy   \n",
              "NN_Model10                  softmax               adam  binary_crossentropy   \n",
              "NN_Model11                  softmax                SGD                hinge   \n",
              "NN_Model12                  softmax               adam  binary_crossentropy   \n",
              "NN_Model13                  softmax                SGD                hinge   \n",
              "NN_Model14                  softmax               adam  binary_crossentropy   \n",
              "NN_Model15                  softmax                SGD                hinge   \n",
              "NN_Model16                  softmax               adam  binary_crossentropy   \n",
              "NN_Model17                  softmax                SGD                hinge   \n",
              "NN_Model18                  softmax               adam  binary_crossentropy   \n",
              "NN_Model19                    swish               adam        squared_hinge   \n",
              "NN_Model20                    swish            RMSprop                hinge   \n",
              "NN_Model21                    swish               adam        squared_hinge   \n",
              "NN_Model22                    swish            RMSprop                hinge   \n",
              "NN_Model23                    swish               adam        squared_hinge   \n",
              "NN_Model24                    swish            RMSprop                hinge   \n",
              "NN_Model25                    swish               adam        squared_hinge   \n",
              "NN_Model26                    swish            RMSprop                hinge   \n",
              "NN_Model27                    swish               adam        squared_hinge   \n",
              "\n",
              "             batch_size  num_of_epochs  training_loss  training_accuracy  \\\n",
              "model_names                                                                \n",
              "NN_Model1           128             10       0.004854           0.992315   \n",
              "NN_Model2           128             10       0.004738           0.992463   \n",
              "NN_Model3           128             10       0.004985           0.992019   \n",
              "NN_Model4           128             10       0.004483           0.992889   \n",
              "NN_Model5           128             10       0.004479           0.993185   \n",
              "NN_Model6           128             10       0.004363           0.993185   \n",
              "NN_Model7           128             10       0.004343           0.992741   \n",
              "NN_Model8           128             10       0.003791           0.994241   \n",
              "NN_Model9           128             10       0.004615           0.992463   \n",
              "NN_Model10          128             10       0.004314           0.993315   \n",
              "NN_Model11          128             10       1.077790           0.330278   \n",
              "NN_Model12          128             10       0.004667           0.992778   \n",
              "NN_Model13          128             10       1.078355           0.182759   \n",
              "NN_Model14          128             10       0.004430           0.992870   \n",
              "NN_Model15          128             10       1.078435           0.212630   \n",
              "NN_Model16          128             10       0.003907           0.994000   \n",
              "NN_Model17          128             10       1.078915           0.198056   \n",
              "NN_Model18          128             10       0.004502           0.992741   \n",
              "NN_Model19          128             10       0.487915           0.981185   \n",
              "NN_Model20          128             10       0.675777           0.977759   \n",
              "NN_Model21          128             10       0.497580           0.977315   \n",
              "NN_Model22          128             10       0.694573           0.974407   \n",
              "NN_Model23          128             10       0.497504           0.978870   \n",
              "NN_Model24          128             10       0.693955           0.973852   \n",
              "NN_Model25          128             10       0.506632           0.977704   \n",
              "NN_Model26          128             10       0.675679           0.977778   \n",
              "NN_Model27          128             10       0.505768           0.977944   \n",
              "\n",
              "             validation_loss  validation_accuracy  test_loss  test_accuracy  \n",
              "model_names                                                                  \n",
              "NN_Model1           0.011905             0.980000   0.016297         0.9764  \n",
              "NN_Model2           0.012179             0.981667   0.015989         0.9775  \n",
              "NN_Model3           0.012793             0.981167   0.014604         0.9787  \n",
              "NN_Model4           0.012221             0.981333   0.015238         0.9804  \n",
              "NN_Model5           0.012013             0.981667   0.014029         0.9797  \n",
              "NN_Model6           0.012088             0.981667   0.015033         0.9804  \n",
              "NN_Model7           0.012231             0.981000   0.016622         0.9771  \n",
              "NN_Model8           0.012957             0.980000   0.015725         0.9788  \n",
              "NN_Model9           0.013242             0.983167   0.015259         0.9778  \n",
              "NN_Model10          0.012950             0.980167   0.016143         0.9765  \n",
              "NN_Model11          1.077535             0.366500   1.077523         0.3647  \n",
              "NN_Model12          0.012754             0.981667   0.013101         0.9822  \n",
              "NN_Model13          1.078171             0.193167   1.078138         0.1971  \n",
              "NN_Model14          0.013016             0.981000   0.015119         0.9792  \n",
              "NN_Model15          1.078178             0.247167   1.078247         0.2286  \n",
              "NN_Model16          0.012220             0.982000   0.014652         0.9804  \n",
              "NN_Model17          1.078746             0.221333   1.078786         0.2109  \n",
              "NN_Model18          0.012079             0.981667   0.013233         0.9810  \n",
              "NN_Model19          0.489006             0.977333   0.490673         0.9770  \n",
              "NN_Model20          0.675434             0.976000   0.676935         0.9748  \n",
              "NN_Model21          0.497425             0.978333   0.499706         0.9746  \n",
              "NN_Model22          0.694674             0.976833   0.695544         0.9738  \n",
              "NN_Model23          0.498778             0.978167   0.500178         0.9732  \n",
              "NN_Model24          0.693781             0.975667   0.694413         0.9738  \n",
              "NN_Model25          0.507458             0.977500   0.509736         0.9720  \n",
              "NN_Model26          0.675963             0.976167   0.676876         0.9728  \n",
              "NN_Model27          0.507822             0.976833   0.508668         0.9728  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best Model without using convoluted layers and computer vision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>first_layer_nodes</th>\n",
              "      <th>first_layer_activation</th>\n",
              "      <th>second_layer_nodes</th>\n",
              "      <th>second_layer_activation</th>\n",
              "      <th>third_layer_nodes</th>\n",
              "      <th>third_layer_activation</th>\n",
              "      <th>output_layer_activation</th>\n",
              "      <th>optimizer_function</th>\n",
              "      <th>loss_function</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>num_of_epochs</th>\n",
              "      <th>training_loss</th>\n",
              "      <th>training_accuracy</th>\n",
              "      <th>validation_loss</th>\n",
              "      <th>validation_accuracy</th>\n",
              "      <th>test_loss</th>\n",
              "      <th>test_accuracy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model_names</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>NN_Model12</th>\n",
              "      <td>128</td>\n",
              "      <td>gelu</td>\n",
              "      <td>175</td>\n",
              "      <td>gelu</td>\n",
              "      <td>225</td>\n",
              "      <td>gelu</td>\n",
              "      <td>softmax</td>\n",
              "      <td>adam</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>128</td>\n",
              "      <td>10</td>\n",
              "      <td>0.004667</td>\n",
              "      <td>0.992778</td>\n",
              "      <td>0.012754</td>\n",
              "      <td>0.981667</td>\n",
              "      <td>0.013101</td>\n",
              "      <td>0.9822</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             first_layer_nodes first_layer_activation  second_layer_nodes  \\\n",
              "model_names                                                                 \n",
              "NN_Model12                 128                   gelu                 175   \n",
              "\n",
              "            second_layer_activation  third_layer_nodes third_layer_activation  \\\n",
              "model_names                                                                     \n",
              "NN_Model12                     gelu                225                   gelu   \n",
              "\n",
              "            output_layer_activation optimizer_function        loss_function  \\\n",
              "model_names                                                                   \n",
              "NN_Model12                  softmax               adam  binary_crossentropy   \n",
              "\n",
              "             batch_size  num_of_epochs  training_loss  training_accuracy  \\\n",
              "model_names                                                                \n",
              "NN_Model12          128             10       0.004667           0.992778   \n",
              "\n",
              "             validation_loss  validation_accuracy  test_loss  test_accuracy  \n",
              "model_names                                                                  \n",
              "NN_Model12          0.012754             0.981667   0.013101         0.9822  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def return_best_model(dataframe : pd.DataFrame):    \n",
        "    max_value = dataframe.test_accuracy.sort_values(ascending = False)[0]\n",
        "    return dataframe[dataframe.test_accuracy == max_value]\n",
        "\n",
        "return_best_model(df)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
